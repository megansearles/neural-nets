{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's my implementation of a regular feed-forward network.  I've implemented it as a class called ```neural_net```.  When it's called it takes as arguments a list of integers, which specifies the number of nodes at each layer.  For example, ```neural_net(5,7,3)``` creates a neural network with 5 input neurons, a single hidden layer with 7 neurons, and 3 outpute neurons.  ```neural_net(5,5,5,5,5)``` is a neural net with input, output, and 3 hidden layers, with 5 neurons at each layer.\n",
    "\n",
    "Since there's a number of ways to set things up (and mine might be slightly different than the way you did it), I'll explain here the details of my setup.  Each data point (training or testing) will be in the form of a column vector.  For example, let $X$ be a single input vector with $n$ features.  To find the resulting activations at the second layer of neurons (the first hidden layer) we multiply $X$ by the weight matrix $W$.  If there are $n$ input neurons and $m$ neurons in the hidden layer, then $W$ will be $m$ by $n$.  Thus, the values of the activations at the first hidden layer (before applying the activation function) is just given by the matrix product $WX$.  So to find the value of the activation at the first neuron in the hidden layer, we take the first row of $W$ and take the dot product with the vector $X$, the activation at the second neuron is obtained by taking the second row of $W$ and dotting with the vector $X$, and so on.\n",
    "\n",
    "Doing it like this, we could incorporate bias terms explicitly, but adding on a separate vector of biases, say $B$, to $WX$, but in the code below I've implemented it slightly differently.  Instead of adding the bias vector $B$ onto $WX$, I insert the $B$ into the front of the matrix $W$, so that it is now an $m$ by $n+1$ matrix with first column $B$, and I turn $X$ into a $n+1$ dimensional vector, by adding a 1 in the first spot (notice that the dimensions still work out for the matrix multiplication).  It shouldn't be hard to convince yourself that modifying $X$ and $W$ in this way is the same thing as adding on the bias terms separately.\n",
    "\n",
    "We can do the same after applying the activation function when going from the first hidden layer to the next layer, by using a matrix $W$ with one extra column (corresponding to the biases).  Notice though that this requires the extra step of putting an extra 1 in the first spot of the neuron vector at each layer before multiplying by $W$.\n",
    "\n",
    "One final thing, note that in the discussion above $X$ could have consisted of $k$ observations, in which case it would have been an $n \\times k$ matrix.  In this case, the formulas all hold the same, but at each layer you'll have a list of $k$ vectors which contain the values of the corresponding neurons for each of the $k$ input vectors.  Instead of adding a single 1 to the first position to account for the biases, we'll have to add a row of 1s at each step to the matrix $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import pandas as pd\n",
    "from sklearn import cross_validation\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class neural_net:\n",
    "    \n",
    "    # The __init__ function is called whenever an instance of the class is created.\n",
    "    # The argument *argv represents the list of integers that are passed to the class when the instance is created.\n",
    "    \n",
    "    def __init__(self,*argv):\n",
    "        self.layer_sizes=argv  # List of layer sizes, starting with the input layer and ending with output layer.\n",
    "        self.w={}              # Empty dictionary which we will fill up with the weight matrices.\n",
    "        # Next we define the weight matrices, so that the ith weight matrix is called self.w[i].\n",
    "        # I read somewhere that it's a good idea to initialize your weight so that they are uniformly \n",
    "        # distributed in the interval [-b,b], where b is some value depending on the number of neurons \n",
    "        # in the incoming and outgoing layers (see the definition below).  Notice in the size of the \n",
    "        # matrix I'm adding an extra column for the biases (it's self.layer_sizes[i+1] by self.layer_sizes[i]+1).\n",
    "        for i in range(len(self.layer_sizes)-1):\n",
    "            b=np.sqrt(6/(self.layer_sizes[i]+self.layer_sizes[i+1]))\n",
    "            self.w[i]=np.random.uniform(-b,b,(self.layer_sizes[i+1],self.layer_sizes[i]+1))\n",
    "    \n",
    "    \n",
    "    # The reset function below is used by the perf_test functions below, and just re-initializes the weights\n",
    "    # in exactly the same way the __init__ function does above.\n",
    "        \n",
    "    def reset(self,*argv):\n",
    "        self.layer_sizes=argv  \n",
    "        self.w={}              \n",
    "        for i in range(len(self.layer_sizes)-1):\n",
    "            self.w[i]=np.random.normal(0,1,(self.layer_sizes[i+1],self.layer_sizes[i]+1))\n",
    "            \n",
    "            \n",
    "    # Sigmoid function.\n",
    "        \n",
    "    def sigmoid(self,t):\n",
    "        return 1.0/(1+np.exp(-t))\n",
    "    \n",
    "    \n",
    "    # Function d_sigma(t)=t(1-t).\n",
    "    \n",
    "    def d_sigmoid(self,t):\n",
    "        return t*(1.0-t)\n",
    "    \n",
    "    \n",
    "    # The predict function takes a column vector input X (or matrix of column vectors) and feeds it forward through\n",
    "    # the network, computing the activations at each neuron.  It returns the values of the output neurons as a column \n",
    "    # vector (if there is only one input), or as a matrix of k column vectors if there were k columns in X.\n",
    "        \n",
    "    def predict(self,X):\n",
    "        self.number_training=X.shape[1]              # Number of data points to feed into the network.\n",
    "        bias_row=np.ones(self.number_training)       # A row of ones which will be stacked on top of X.\n",
    "        # Define two empty dictionaries, which will hold the values of the activations at each neuron. \n",
    "        # self.a holds the values of the activations before applying the activation function, while self.z holds\n",
    "        # the values that are returned from the activation function. \n",
    "        self.a={}                                                                                        \n",
    "        self.z={}                                    \n",
    "        self.z[0]=np.vstack((bias_row,X))\n",
    "        # The self.a values are obtained by multiplying the self.z of the previous layer by the corresponging self.w\n",
    "        # matrices.  The self.z values are obtained from these by applying the activation function, and then stacking\n",
    "        # a row of ones on top.\n",
    "        for i in range(len(self.layer_sizes)-2):\n",
    "            self.a[i+1]=self.w[i]@self.z[i]\n",
    "            self.z[i+1]=np.vstack((bias_row,self.sigmoid(self.a[i+1])))\n",
    "        # The last (ouptut) layer doesn't need a row of ones stacked on top (since we won't be multiplying by weights\n",
    "        # and adding bias terms, so we handle this layer separately, and return the self.z values from the last layer.\n",
    "        self.a[len(self.layer_sizes)-1]=self.w[len(self.layer_sizes)-2]@self.z[len(self.layer_sizes)-2]    \n",
    "        self.z[len(self.layer_sizes)-1]=self.sigmoid(self.a[len(self.layer_sizes)-1]) \n",
    "        return self.z[len(self.z)-1]\n",
    "    \n",
    "\n",
    "    # The compute_delta function takes as input the training data points X and corresponding target variables T, and \n",
    "    # computes the corresponding delta values needed for back-propogation.  As set-up here, it computes these values \n",
    "    # for the cross-entropy error function, but could be changed easily to use other error functions.\n",
    "    \n",
    "    def compute_delta(self,X,T):\n",
    "        self.delta={}\n",
    "        # The ouput layer deltas are just the output values self.z[len(self.z)-1] minus the target variables T.\n",
    "        self.delta[len(self.layer_sizes)-1]=self.z[len(self.z)-1]-T\n",
    "        derivatives={}\n",
    "        # Delta values at earlier layers are computed recursively from the formulas.  Because we've added a row\n",
    "        # to each self.z to accomodate the bias terms, going backwards we must drop a row at each stage.\n",
    "        for i in range(len(self.layer_sizes)-2):\n",
    "            derivatives[len(self.layer_sizes)-2-i]=self.d_sigmoid(self.z[len(self.layer_sizes)-2-i])\n",
    "            self.delta[len(self.layer_sizes)-2-i]=np.delete(((self.w[len(self.layer_sizes)-2-i].T)@self.delta[len(self.layer_sizes)-1-i])*derivatives[len(self.layer_sizes)-2-i],0,0)\n",
    "        # Return the dictionary of delta values which will be used by backprop function to compute the gradients.    \n",
    "        return self.delta  \n",
    "    \n",
    "    \n",
    "    # The backprop function uses the predict and compute_deltas functions to compute the gradient of the cross-entropy\n",
    "    # error function with respect to the weights self.w at single training point, and then averaging them to get the \n",
    "    # value of the gradient over all training points.\n",
    "    \n",
    "    def backprop(self,X,T):           \n",
    "        self.predict(X)\n",
    "        deltas=self.compute_delta(X,T)\n",
    "        grad={}\n",
    "        # grad_avg will be a list which contains the running average of the gradients as we run over the list of training\n",
    "        # points.  Since we are taking the gradient with respect to the weights self.w, which form a list of matrices, \n",
    "        # we write the resulting gradients as matrices of the same sizes as the self.w matrices.\n",
    "        grad_avg={}\n",
    "        # The counter i runs over the number of weight matrices, and initializes each average gradient matrix as zero matrices.\n",
    "        for i in range(len(self.layer_sizes)-1):\n",
    "            grad_avg[i]=np.zeros((self.layer_sizes[i+1],self.layer_sizes[i]+1))\n",
    "        # The counter m ranges over the set of all training points.  For each point it uses the deltas from the \n",
    "        # compute_deltas function and the self.z values to compute the required derivatives and fill up the gradient\n",
    "        # matrices.  It then divides these matrices by the number of training data points, before adding them to the \n",
    "        # running averages.\n",
    "        for m in range(X.shape[1]):\n",
    "            grad[m]={}\n",
    "            for i in range(len(self.layer_sizes)-1):\n",
    "                grad[m][i]=np.copy(self.w[i])\n",
    "                for k in range(self.w[i].shape[0]):\n",
    "                    for j in range(self.w[i].shape[1]):\n",
    "                        grad[m][i][k,j]=deltas[i+1][k,m]*self.z[i][j,m]\n",
    "                grad_avg[i]+=(1/X.shape[1])*grad[m][i]\n",
    "        return grad_avg\n",
    "    \n",
    "    \n",
    "    # The fit function takes the training data X and target values T, along with a learning_rate and number of epochs, \n",
    "    # and fits the network to the data.  \n",
    "    \n",
    "    def fit(self,X,T,learning_rate,epochs):\n",
    "        # As the counter i runs over the number of epochs, it updates the weight matrices self.w by recomputing the \n",
    "        # gradient, and subtracting it from the weights (times the learning rate).\n",
    "        for i in range(epochs):\n",
    "            self.grad_avg=self.backprop(X,T)\n",
    "            for j in range(len(self.layer_sizes)-1):\n",
    "                self.w[j]-=learning_rate*self.grad_avg[j]\n",
    "    \n",
    "    \n",
    "    # The following three functions are for use when the network is being used to predict binary outcome variables (such\n",
    "    # as the titanic problem, when we are trying to classify passengers as having survived (1) or perished (0).  It won't\n",
    "    # work for regression problems (when we are trying to predict the values of some function) though it could be easily \n",
    "    # modified to do so.\n",
    "    \n",
    "                \n",
    "    # The score function takes training data X and target values T, and determines how accurate the network predicts the\n",
    "    # values of T based on the features in X.  It rounds the predicted values to 0 or 1, returns the percentage of \n",
    "    # correct predictions.\n",
    "                \n",
    "    def score(self,X,T):\n",
    "        return 1-(np.abs((np.round_(self.predict(X))-T)).sum())/len(X.T)\n",
    "    \n",
    "    \n",
    "    # The functions perf_test and perf_test2 are similar.  Both of them take as input some training data X along with \n",
    "    # target variables T, a value specifying the number of epochs, a list of values which contains the different learning\n",
    "    # rates to test, a list (or two, in the case of perf_test2) of values containing the number of neurons to try for the \n",
    "    # hidden layer(s), and a fold number.  \n",
    "    \n",
    "    # For each different learning rate, and possible combination of hidden neurons from the lists supplied, perf_test \n",
    "    # and perf_test2 divides the training data into fold_number different sets, trains the data on all but one of these\n",
    "    # sets, and tests the networks performance on the remaining \"hold-out\" data set.  The performance of the network is\n",
    "    # recorded at each epoch.  The process is repeated for each different hold-out chuck of the data, retraining the \n",
    "    # network and recording it's performance at every epoch.\n",
    "    \n",
    "    # In this way we can test a whole range of different network configurations (neuron numbers, learning rates, epochs), \n",
    "    # to see which combinations give the best performance.  The only difference between perf_test and perf_test2 is that\n",
    "    # perf_test2 trains networks with 2 hidden layers, while perf_test trains networks with only one hidden layer.\n",
    "    \n",
    "    def perf_test(self,X,T,epoch_number,learning_rate_range,neuron_range,fold_number=5):\n",
    "        self.row_position=0\n",
    "        number_of_rows=len(learning_rate_range)*len(neuron_range)*fold_number\n",
    "        self.perf_data=pd.DataFrame(index=list(range(number_of_rows)),columns=['hidden neurons','learning parameter']+list(range(epoch_number+1)))\n",
    "        kfolds=cross_validation.KFold(len(X.T),fold_number)\n",
    "        for jjj in neuron_range:\n",
    "            for iii in learning_rate_range:\n",
    "                for train,test in kfolds:\n",
    "                    self.perf_data.loc[self.row_position,'hidden neurons']=jjj\n",
    "                    self.perf_data.loc[self.row_position,'learning parameter']=iii\n",
    "                    self.reset(len(X.T[0]),jjj,1)\n",
    "                    self.predict(X[:,train])\n",
    "                    self.perf_data.loc[self.row_position,0]=self.score(X[:,test],T[:,test])\n",
    "                    for run in range(epoch_number):\n",
    "                        self.grad_avg=self.backprop(X[:,train],T[:,train])\n",
    "                        for j in range(len(self.layer_sizes)-1):\n",
    "                            self.w[j]-=iii*self.grad_avg[j]\n",
    "                        self.perf_data.loc[self.row_position,run+1]=self.score(X[:,test],T[:,test])\n",
    "                    self.row_position+=1\n",
    "                print('hidden neurons=',jjj,\"\\t learning parameter=\",iii)\n",
    "        return self.perf_data\n",
    "    \n",
    "    \n",
    "    def perf_test2(self,X,T,epoch_number,learning_rate_range,neuron_range1,neuron_range2,fold_number=5):\n",
    "        self.row_position=0\n",
    "        number_of_rows=len(learning_rate_range)*len(neuron_range1)*len(neuron_range2)*fold_number\n",
    "        self.perf_data=pd.DataFrame(index=list(range(number_of_rows)),columns=['hidden neurons 1','hidden neurons 2','learning parameter']+list(range(epoch_number+1)))\n",
    "        kfolds=cross_validation.KFold(len(X.T),fold_number)\n",
    "        for jjj in neuron_range1:\n",
    "            for kkk in neuron_range1:\n",
    "                for iii in learning_rate_range:\n",
    "                    for train,test in kfolds:\n",
    "                        self.perf_data.loc[self.row_position,'hidden neurons 1']=jjj\n",
    "                        self.perf_data.loc[self.row_position,'hidden neurons 2']=kkk\n",
    "                        self.perf_data.loc[self.row_position,'learning parameter']=iii\n",
    "                        self.reset(len(X.T[0]),jjj,kkk,1)\n",
    "                        self.predict(X[:,train])\n",
    "                        self.perf_data.loc[self.row_position,0]=self.score(X[:,test],T[:,test])\n",
    "                        for run in range(epoch_number):\n",
    "                            self.grad_avg=self.backprop(X[:,train],T[:,train])\n",
    "                            for j in range(len(self.layer_sizes)-1):\n",
    "                                self.w[j]-=iii*self.grad_avg[j]\n",
    "                            self.perf_data.loc[self.row_position,run+1]=self.score(X[:,test],T[:,test])\n",
    "                        self.row_position+=1\n",
    "                    print('hidden neurons 1 =',jjj,'\\t hidden neurons 2 =',kkk,\"\\t learning parameter=\",iii)\n",
    "        return self.perf_data\n",
    "                        \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a really dumb first check to see that the neural network does what it's supposed to, lets construct a simple one and train it on a single input vector $X$ with target vector $T$, and then pass $X$ back through to see that we get something close to $T$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_pract=np.array([[0.1,0.8,0.5]]).T     # Remember to take the transpose, since we want them to be column vectors.\n",
    "T_pract=np.array([[0.6,0.9,0.15,0.18]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initializing the network, with one hidden layer containing 5 neurons.\n",
    "\n",
    "net_pract=neural_net(3,5,4)                        \n",
    "\n",
    "\n",
    "# Training the network to our single trianing point, using learning rate 0.1 AND 200 epochs.\n",
    "      \n",
    "net_pract.fit(X_pract,T_pract,0.1,200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now pass the training vector back through to see if we get something similar to ```T_pract```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.60000052],\n",
       "       [ 0.8990522 ],\n",
       "       [ 0.14982862],\n",
       "       [ 0.18009814]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_pract.predict(X_pract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is pretty close to ```T_pract```, and hence we know with 100% certainty that there are absolutely no bugs in the above code.\n",
    "\n",
    "Now let's look at making predictions with the titanic data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df=pd.read_csv('titanic_train.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function for formatting data.\n",
    "\n",
    "def format_data(datafile,predictors):\n",
    "    titanic=pd.read_csv(datafile)\n",
    "    # Change 'male' and 'female' in the 'Sex' column to 0 and 1 respectively.\n",
    "    titanic.loc[titanic['Sex']=='male','Sex']=0\n",
    "    titanic.loc[titanic['Sex']=='female','Sex']=1\n",
    "    # Creates columns 'Emb1' through 'Emb3' which will contain the point the passenger embarked at.  I.e. a passenger \n",
    "    # who embarks at 'C' will have a 1 in 'Emb1' column and 0 in 'Emb2' and 'Emb3', passengers who embarked at 'S' will\n",
    "    # have a 1 in 'Emb2', and so on.\n",
    "    titanic['Emb1']=0\n",
    "    titanic['Emb2']=0\n",
    "    titanic['Emb3']=0\n",
    "    titanic.loc[titanic['Embarked']=='C','Emb1']=1\n",
    "    titanic.loc[titanic['Embarked']=='S','Emb2']=1\n",
    "    titanic.loc[titanic['Embarked']=='Q','Emb3']=1\n",
    "    # Dropping the original 'Embarked' column.\n",
    "    titanic=titanic.drop('Embarked',axis=1)\n",
    "    # Drop any columns whose name is not in the list 'predictors'.\n",
    "    for val in titanic.columns.values:\n",
    "        if not(val in predictors+['Survived']):\n",
    "            titanic=titanic.drop(val,axis=1)\n",
    "    # In the columns from the list 'predictors', replace and NAN values with the median of the values in that column,\n",
    "    # and normalize the data so that each feature has zero mean and unit variance.  This is not technically necessary\n",
    "    # for the neural network to function, but if there are large values it will take much longer to train, so it's\n",
    "    # better to normalize.\n",
    "    for val in predictors:\n",
    "        titanic[val] = titanic[val].fillna(titanic[val].median())\n",
    "        titanic[val] = (titanic[val]-titanic[val].mean())/np.sqrt(titanic[val].var())\n",
    "    return titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictors=['Sex', 'Age', 'Fare', 'Pclass', 'SibSp', 'Parch','Emb1','Emb2','Emb3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Emb1</th>\n",
       "      <th>Emb2</th>\n",
       "      <th>Emb3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.826913</td>\n",
       "      <td>-0.737281</td>\n",
       "      <td>-0.565419</td>\n",
       "      <td>0.432550</td>\n",
       "      <td>-0.473408</td>\n",
       "      <td>-0.502163</td>\n",
       "      <td>-0.481772</td>\n",
       "      <td>0.618959</td>\n",
       "      <td>-0.30739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.565228</td>\n",
       "      <td>1.354813</td>\n",
       "      <td>0.663488</td>\n",
       "      <td>0.432550</td>\n",
       "      <td>-0.473408</td>\n",
       "      <td>0.786404</td>\n",
       "      <td>2.073341</td>\n",
       "      <td>-1.613803</td>\n",
       "      <td>-0.30739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.826913</td>\n",
       "      <td>1.354813</td>\n",
       "      <td>-0.258192</td>\n",
       "      <td>-0.474279</td>\n",
       "      <td>-0.473408</td>\n",
       "      <td>-0.488580</td>\n",
       "      <td>-0.481772</td>\n",
       "      <td>0.618959</td>\n",
       "      <td>-0.30739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.565228</td>\n",
       "      <td>1.354813</td>\n",
       "      <td>0.433068</td>\n",
       "      <td>0.432550</td>\n",
       "      <td>-0.473408</td>\n",
       "      <td>0.420494</td>\n",
       "      <td>-0.481772</td>\n",
       "      <td>0.618959</td>\n",
       "      <td>-0.30739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.826913</td>\n",
       "      <td>-0.737281</td>\n",
       "      <td>0.433068</td>\n",
       "      <td>-0.474279</td>\n",
       "      <td>-0.473408</td>\n",
       "      <td>-0.486064</td>\n",
       "      <td>-0.481772</td>\n",
       "      <td>0.618959</td>\n",
       "      <td>-0.30739</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived    Pclass       Sex       Age     SibSp     Parch      Fare  \\\n",
       "0         0  0.826913 -0.737281 -0.565419  0.432550 -0.473408 -0.502163   \n",
       "1         1 -1.565228  1.354813  0.663488  0.432550 -0.473408  0.786404   \n",
       "2         1  0.826913  1.354813 -0.258192 -0.474279 -0.473408 -0.488580   \n",
       "3         1 -1.565228  1.354813  0.433068  0.432550 -0.473408  0.420494   \n",
       "4         0  0.826913 -0.737281  0.433068 -0.474279 -0.473408 -0.486064   \n",
       "\n",
       "       Emb1      Emb2     Emb3  \n",
       "0 -0.481772  0.618959 -0.30739  \n",
       "1  2.073341 -1.613803 -0.30739  \n",
       "2 -0.481772  0.618959 -0.30739  \n",
       "3 -0.481772  0.618959 -0.30739  \n",
       "4 -0.481772  0.618959 -0.30739  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df=format_data('titanic_train.csv',predictors)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert the pandas data frame train_df into a set of feature vectors X and target variables T.  In the data frame \n",
    "# each passenger is a row, while we want each passenger to be a column vector of X, so we neet to take the transpose\n",
    "# for both X and T (and we need to reshape T so that it's a 1-dimensional array).\n",
    "\n",
    "X=train_df[predictors].values.T\n",
    "T=np.reshape(train_df['Survived'].values.T,(1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to use the ```perf_test``` functions to try to determine which parameters (number of hidden neurons, learning rate, number of epochs) to use for our model.  Obviously, the more parameters you test the longer it will take.  I've run this a couple of times with wide ranges of parameters, and it seems that a good range of neurons to try using would be 16-20, and some good candidates for the learning parameter might be 1.2,1.4 and 1.6.\n",
    "\n",
    "To test these, we need to initialize a neural network which we'll call ```net```.  It turns out with the way that the function ```perf_test``` is written, it doesn't matter the initial configuration of neurons we pick (it will always select the right number of input neurons, a single output neuron, and let the number of hidden neurons range according to the parameters):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net=neural_net(1,1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the 50 below is the number of epochs used in each test, and the 5 is the number of k-folds to use for each combination of learning rate and neuron count.  This means that for each combination of neuron number and learning rate, it will train and test 5 different neural networks using those parameters, on different subsets of the training data, and will record how well each of these networks performs at each of the 50 training epochs.  When I did this before, I used 200 epochs and 10 k-folds, but I didn't want this to take forever:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden neurons= 16 \t learning parameter= 1.2\n",
      "hidden neurons= 16 \t learning parameter= 1.4\n",
      "hidden neurons= 16 \t learning parameter= 1.6\n",
      "hidden neurons= 18 \t learning parameter= 1.2\n",
      "hidden neurons= 18 \t learning parameter= 1.4\n",
      "hidden neurons= 18 \t learning parameter= 1.6\n",
      "hidden neurons= 20 \t learning parameter= 1.2\n",
      "hidden neurons= 20 \t learning parameter= 1.4\n",
      "hidden neurons= 20 \t learning parameter= 1.6\n"
     ]
    }
   ],
   "source": [
    "performance_data=net.perf_test(X,T,50,[1.2,1.4,1.6],[16,18,20],5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save the performance date as a csv in case the jupyter session ends unexpectedly.\n",
    "\n",
    "performance_data.to_csv('cross validation data (practice).csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at how ```perf_test``` returns the data.  For each learning rate and number of hidden neurons, it returns a list of 51 scores, which reveal how well the network performed on the hold-out data after each training epoch.  Notice that each combination of parameters was tested 5 times (fold_number=5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hidden neurons</th>\n",
       "      <th>learning parameter</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.670391</td>\n",
       "      <td>0.681564</td>\n",
       "      <td>0.720670</td>\n",
       "      <td>0.765363</td>\n",
       "      <td>0.731844</td>\n",
       "      <td>0.720670</td>\n",
       "      <td>0.737430</td>\n",
       "      <td>0.754190</td>\n",
       "      <td>...</td>\n",
       "      <td>0.793296</td>\n",
       "      <td>0.793296</td>\n",
       "      <td>0.793296</td>\n",
       "      <td>0.793296</td>\n",
       "      <td>0.793296</td>\n",
       "      <td>0.793296</td>\n",
       "      <td>0.793296</td>\n",
       "      <td>0.793296</td>\n",
       "      <td>0.793296</td>\n",
       "      <td>0.793296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.539326</td>\n",
       "      <td>0.584270</td>\n",
       "      <td>0.640449</td>\n",
       "      <td>0.674157</td>\n",
       "      <td>0.685393</td>\n",
       "      <td>0.719101</td>\n",
       "      <td>0.724719</td>\n",
       "      <td>0.758427</td>\n",
       "      <td>...</td>\n",
       "      <td>0.831461</td>\n",
       "      <td>0.831461</td>\n",
       "      <td>0.831461</td>\n",
       "      <td>0.831461</td>\n",
       "      <td>0.831461</td>\n",
       "      <td>0.831461</td>\n",
       "      <td>0.831461</td>\n",
       "      <td>0.831461</td>\n",
       "      <td>0.831461</td>\n",
       "      <td>0.831461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.612360</td>\n",
       "      <td>0.612360</td>\n",
       "      <td>0.617978</td>\n",
       "      <td>0.707865</td>\n",
       "      <td>0.730337</td>\n",
       "      <td>0.735955</td>\n",
       "      <td>0.758427</td>\n",
       "      <td>0.764045</td>\n",
       "      <td>...</td>\n",
       "      <td>0.780899</td>\n",
       "      <td>0.780899</td>\n",
       "      <td>0.780899</td>\n",
       "      <td>0.780899</td>\n",
       "      <td>0.780899</td>\n",
       "      <td>0.780899</td>\n",
       "      <td>0.780899</td>\n",
       "      <td>0.780899</td>\n",
       "      <td>0.780899</td>\n",
       "      <td>0.780899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.595506</td>\n",
       "      <td>0.674157</td>\n",
       "      <td>0.730337</td>\n",
       "      <td>0.730337</td>\n",
       "      <td>0.741573</td>\n",
       "      <td>0.741573</td>\n",
       "      <td>0.758427</td>\n",
       "      <td>0.758427</td>\n",
       "      <td>...</td>\n",
       "      <td>0.775281</td>\n",
       "      <td>0.775281</td>\n",
       "      <td>0.775281</td>\n",
       "      <td>0.775281</td>\n",
       "      <td>0.775281</td>\n",
       "      <td>0.775281</td>\n",
       "      <td>0.775281</td>\n",
       "      <td>0.775281</td>\n",
       "      <td>0.775281</td>\n",
       "      <td>0.775281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.735955</td>\n",
       "      <td>0.741573</td>\n",
       "      <td>0.747191</td>\n",
       "      <td>0.752809</td>\n",
       "      <td>0.769663</td>\n",
       "      <td>0.769663</td>\n",
       "      <td>0.769663</td>\n",
       "      <td>0.769663</td>\n",
       "      <td>...</td>\n",
       "      <td>0.820225</td>\n",
       "      <td>0.820225</td>\n",
       "      <td>0.820225</td>\n",
       "      <td>0.820225</td>\n",
       "      <td>0.820225</td>\n",
       "      <td>0.820225</td>\n",
       "      <td>0.820225</td>\n",
       "      <td>0.814607</td>\n",
       "      <td>0.814607</td>\n",
       "      <td>0.814607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>16</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.335196</td>\n",
       "      <td>0.558659</td>\n",
       "      <td>0.586592</td>\n",
       "      <td>0.642458</td>\n",
       "      <td>0.681564</td>\n",
       "      <td>0.709497</td>\n",
       "      <td>0.703911</td>\n",
       "      <td>0.709497</td>\n",
       "      <td>...</td>\n",
       "      <td>0.787709</td>\n",
       "      <td>0.787709</td>\n",
       "      <td>0.787709</td>\n",
       "      <td>0.793296</td>\n",
       "      <td>0.787709</td>\n",
       "      <td>0.787709</td>\n",
       "      <td>0.787709</td>\n",
       "      <td>0.787709</td>\n",
       "      <td>0.787709</td>\n",
       "      <td>0.787709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>16</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.516854</td>\n",
       "      <td>0.634831</td>\n",
       "      <td>0.747191</td>\n",
       "      <td>0.747191</td>\n",
       "      <td>0.780899</td>\n",
       "      <td>0.780899</td>\n",
       "      <td>0.780899</td>\n",
       "      <td>0.780899</td>\n",
       "      <td>...</td>\n",
       "      <td>0.764045</td>\n",
       "      <td>0.764045</td>\n",
       "      <td>0.764045</td>\n",
       "      <td>0.764045</td>\n",
       "      <td>0.764045</td>\n",
       "      <td>0.764045</td>\n",
       "      <td>0.764045</td>\n",
       "      <td>0.764045</td>\n",
       "      <td>0.764045</td>\n",
       "      <td>0.764045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.595506</td>\n",
       "      <td>0.668539</td>\n",
       "      <td>0.719101</td>\n",
       "      <td>0.730337</td>\n",
       "      <td>0.758427</td>\n",
       "      <td>0.758427</td>\n",
       "      <td>0.769663</td>\n",
       "      <td>0.780899</td>\n",
       "      <td>...</td>\n",
       "      <td>0.808989</td>\n",
       "      <td>0.808989</td>\n",
       "      <td>0.803371</td>\n",
       "      <td>0.803371</td>\n",
       "      <td>0.803371</td>\n",
       "      <td>0.803371</td>\n",
       "      <td>0.803371</td>\n",
       "      <td>0.803371</td>\n",
       "      <td>0.803371</td>\n",
       "      <td>0.803371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>16</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.382022</td>\n",
       "      <td>0.584270</td>\n",
       "      <td>0.589888</td>\n",
       "      <td>0.629213</td>\n",
       "      <td>0.640449</td>\n",
       "      <td>0.668539</td>\n",
       "      <td>0.713483</td>\n",
       "      <td>0.719101</td>\n",
       "      <td>...</td>\n",
       "      <td>0.764045</td>\n",
       "      <td>0.764045</td>\n",
       "      <td>0.764045</td>\n",
       "      <td>0.764045</td>\n",
       "      <td>0.764045</td>\n",
       "      <td>0.764045</td>\n",
       "      <td>0.764045</td>\n",
       "      <td>0.764045</td>\n",
       "      <td>0.764045</td>\n",
       "      <td>0.758427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>16</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.646067</td>\n",
       "      <td>0.657303</td>\n",
       "      <td>0.612360</td>\n",
       "      <td>0.679775</td>\n",
       "      <td>0.719101</td>\n",
       "      <td>0.724719</td>\n",
       "      <td>0.792135</td>\n",
       "      <td>0.775281</td>\n",
       "      <td>...</td>\n",
       "      <td>0.831461</td>\n",
       "      <td>0.831461</td>\n",
       "      <td>0.831461</td>\n",
       "      <td>0.831461</td>\n",
       "      <td>0.831461</td>\n",
       "      <td>0.831461</td>\n",
       "      <td>0.831461</td>\n",
       "      <td>0.831461</td>\n",
       "      <td>0.825843</td>\n",
       "      <td>0.825843</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   hidden neurons  learning parameter         0         1         2         3  \\\n",
       "0              16                 1.2  0.670391  0.681564  0.720670  0.765363   \n",
       "1              16                 1.2  0.539326  0.584270  0.640449  0.674157   \n",
       "2              16                 1.2  0.612360  0.612360  0.617978  0.707865   \n",
       "3              16                 1.2  0.595506  0.674157  0.730337  0.730337   \n",
       "4              16                 1.2  0.735955  0.741573  0.747191  0.752809   \n",
       "5              16                 1.4  0.335196  0.558659  0.586592  0.642458   \n",
       "6              16                 1.4  0.516854  0.634831  0.747191  0.747191   \n",
       "7              16                 1.4  0.595506  0.668539  0.719101  0.730337   \n",
       "8              16                 1.4  0.382022  0.584270  0.589888  0.629213   \n",
       "9              16                 1.4  0.646067  0.657303  0.612360  0.679775   \n",
       "\n",
       "          4         5         6         7    ...           41        42  \\\n",
       "0  0.731844  0.720670  0.737430  0.754190    ...     0.793296  0.793296   \n",
       "1  0.685393  0.719101  0.724719  0.758427    ...     0.831461  0.831461   \n",
       "2  0.730337  0.735955  0.758427  0.764045    ...     0.780899  0.780899   \n",
       "3  0.741573  0.741573  0.758427  0.758427    ...     0.775281  0.775281   \n",
       "4  0.769663  0.769663  0.769663  0.769663    ...     0.820225  0.820225   \n",
       "5  0.681564  0.709497  0.703911  0.709497    ...     0.787709  0.787709   \n",
       "6  0.780899  0.780899  0.780899  0.780899    ...     0.764045  0.764045   \n",
       "7  0.758427  0.758427  0.769663  0.780899    ...     0.808989  0.808989   \n",
       "8  0.640449  0.668539  0.713483  0.719101    ...     0.764045  0.764045   \n",
       "9  0.719101  0.724719  0.792135  0.775281    ...     0.831461  0.831461   \n",
       "\n",
       "         43        44        45        46        47        48        49  \\\n",
       "0  0.793296  0.793296  0.793296  0.793296  0.793296  0.793296  0.793296   \n",
       "1  0.831461  0.831461  0.831461  0.831461  0.831461  0.831461  0.831461   \n",
       "2  0.780899  0.780899  0.780899  0.780899  0.780899  0.780899  0.780899   \n",
       "3  0.775281  0.775281  0.775281  0.775281  0.775281  0.775281  0.775281   \n",
       "4  0.820225  0.820225  0.820225  0.820225  0.820225  0.814607  0.814607   \n",
       "5  0.787709  0.793296  0.787709  0.787709  0.787709  0.787709  0.787709   \n",
       "6  0.764045  0.764045  0.764045  0.764045  0.764045  0.764045  0.764045   \n",
       "7  0.803371  0.803371  0.803371  0.803371  0.803371  0.803371  0.803371   \n",
       "8  0.764045  0.764045  0.764045  0.764045  0.764045  0.764045  0.764045   \n",
       "9  0.831461  0.831461  0.831461  0.831461  0.831461  0.831461  0.825843   \n",
       "\n",
       "         50  \n",
       "0  0.793296  \n",
       "1  0.831461  \n",
       "2  0.780899  \n",
       "3  0.775281  \n",
       "4  0.814607  \n",
       "5  0.787709  \n",
       "6  0.764045  \n",
       "7  0.803371  \n",
       "8  0.758427  \n",
       "9  0.825843  \n",
       "\n",
       "[10 rows x 53 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance_data=pd.read_csv('cross validation data (practice).csv')\n",
    "performance_data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the maximum score is in the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85955056179800005"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance_data.columns=['hidden neurons','learning parameter']+list(range(51)) # I reimported the data from the csv, and pandas treated the column names as strings instead of ints, so I'm renaming the columns as ints.\n",
    "performance_data.loc[:,list(range(51))].max().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a pretty high score.  Unfortunately, we sometimes get high scores by accident, by just chosing a subset of the data to test our network on that is particularly predictable (I've seen some as high as 90 percent).  It's more revealing to look at the averages of each of the tests performed over each neuron number/learning parameter pair.\n",
    "\n",
    "To do this, we create a new data frame which contains the average scores for each epoch and each neuron number/learning rate pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hidden neurons</th>\n",
       "      <th>learning parameter</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.630707</td>\n",
       "      <td>0.658785</td>\n",
       "      <td>0.691325</td>\n",
       "      <td>0.726106</td>\n",
       "      <td>0.731762</td>\n",
       "      <td>0.737393</td>\n",
       "      <td>0.749733</td>\n",
       "      <td>0.760950</td>\n",
       "      <td>...</td>\n",
       "      <td>0.800232</td>\n",
       "      <td>0.800232</td>\n",
       "      <td>0.800232</td>\n",
       "      <td>0.800232</td>\n",
       "      <td>0.800232</td>\n",
       "      <td>0.800232</td>\n",
       "      <td>0.800232</td>\n",
       "      <td>0.799109</td>\n",
       "      <td>0.799109</td>\n",
       "      <td>0.799109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.495129</td>\n",
       "      <td>0.620721</td>\n",
       "      <td>0.651026</td>\n",
       "      <td>0.685795</td>\n",
       "      <td>0.716088</td>\n",
       "      <td>0.728416</td>\n",
       "      <td>0.752018</td>\n",
       "      <td>0.753135</td>\n",
       "      <td>...</td>\n",
       "      <td>0.791250</td>\n",
       "      <td>0.791250</td>\n",
       "      <td>0.790126</td>\n",
       "      <td>0.791243</td>\n",
       "      <td>0.790126</td>\n",
       "      <td>0.790126</td>\n",
       "      <td>0.790126</td>\n",
       "      <td>0.790126</td>\n",
       "      <td>0.789003</td>\n",
       "      <td>0.787879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.566669</td>\n",
       "      <td>0.623928</td>\n",
       "      <td>0.649802</td>\n",
       "      <td>0.704758</td>\n",
       "      <td>0.713778</td>\n",
       "      <td>0.744071</td>\n",
       "      <td>0.754190</td>\n",
       "      <td>0.754177</td>\n",
       "      <td>...</td>\n",
       "      <td>0.788996</td>\n",
       "      <td>0.790120</td>\n",
       "      <td>0.788996</td>\n",
       "      <td>0.788996</td>\n",
       "      <td>0.788996</td>\n",
       "      <td>0.788996</td>\n",
       "      <td>0.788996</td>\n",
       "      <td>0.791243</td>\n",
       "      <td>0.793491</td>\n",
       "      <td>0.794614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.451315</td>\n",
       "      <td>0.628579</td>\n",
       "      <td>0.696918</td>\n",
       "      <td>0.701406</td>\n",
       "      <td>0.728360</td>\n",
       "      <td>0.744065</td>\n",
       "      <td>0.751924</td>\n",
       "      <td>0.765407</td>\n",
       "      <td>...</td>\n",
       "      <td>0.791250</td>\n",
       "      <td>0.792373</td>\n",
       "      <td>0.791250</td>\n",
       "      <td>0.790126</td>\n",
       "      <td>0.790126</td>\n",
       "      <td>0.790126</td>\n",
       "      <td>0.790126</td>\n",
       "      <td>0.791250</td>\n",
       "      <td>0.790132</td>\n",
       "      <td>0.790132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.491777</td>\n",
       "      <td>0.610502</td>\n",
       "      <td>0.615096</td>\n",
       "      <td>0.673398</td>\n",
       "      <td>0.709334</td>\n",
       "      <td>0.730657</td>\n",
       "      <td>0.746369</td>\n",
       "      <td>0.764327</td>\n",
       "      <td>...</td>\n",
       "      <td>0.796855</td>\n",
       "      <td>0.797979</td>\n",
       "      <td>0.797979</td>\n",
       "      <td>0.797979</td>\n",
       "      <td>0.797979</td>\n",
       "      <td>0.796855</td>\n",
       "      <td>0.796855</td>\n",
       "      <td>0.795732</td>\n",
       "      <td>0.795732</td>\n",
       "      <td>0.795732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>18</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.408618</td>\n",
       "      <td>0.667742</td>\n",
       "      <td>0.682336</td>\n",
       "      <td>0.711550</td>\n",
       "      <td>0.713797</td>\n",
       "      <td>0.723903</td>\n",
       "      <td>0.727274</td>\n",
       "      <td>0.747505</td>\n",
       "      <td>...</td>\n",
       "      <td>0.785632</td>\n",
       "      <td>0.784508</td>\n",
       "      <td>0.785626</td>\n",
       "      <td>0.784502</td>\n",
       "      <td>0.785626</td>\n",
       "      <td>0.786749</td>\n",
       "      <td>0.786749</td>\n",
       "      <td>0.786749</td>\n",
       "      <td>0.787873</td>\n",
       "      <td>0.787873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.501494</td>\n",
       "      <td>0.543149</td>\n",
       "      <td>0.607194</td>\n",
       "      <td>0.677905</td>\n",
       "      <td>0.704890</td>\n",
       "      <td>0.718348</td>\n",
       "      <td>0.728441</td>\n",
       "      <td>0.736269</td>\n",
       "      <td>...</td>\n",
       "      <td>0.786768</td>\n",
       "      <td>0.787892</td>\n",
       "      <td>0.787892</td>\n",
       "      <td>0.789015</td>\n",
       "      <td>0.789015</td>\n",
       "      <td>0.789015</td>\n",
       "      <td>0.790139</td>\n",
       "      <td>0.790139</td>\n",
       "      <td>0.790139</td>\n",
       "      <td>0.790139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.558778</td>\n",
       "      <td>0.613828</td>\n",
       "      <td>0.669996</td>\n",
       "      <td>0.710426</td>\n",
       "      <td>0.733984</td>\n",
       "      <td>0.755339</td>\n",
       "      <td>0.758684</td>\n",
       "      <td>0.771038</td>\n",
       "      <td>...</td>\n",
       "      <td>0.804720</td>\n",
       "      <td>0.803597</td>\n",
       "      <td>0.803597</td>\n",
       "      <td>0.804720</td>\n",
       "      <td>0.804720</td>\n",
       "      <td>0.804720</td>\n",
       "      <td>0.804720</td>\n",
       "      <td>0.804720</td>\n",
       "      <td>0.804720</td>\n",
       "      <td>0.804720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.459174</td>\n",
       "      <td>0.603754</td>\n",
       "      <td>0.624079</td>\n",
       "      <td>0.679047</td>\n",
       "      <td>0.704921</td>\n",
       "      <td>0.731800</td>\n",
       "      <td>0.727368</td>\n",
       "      <td>0.753135</td>\n",
       "      <td>...</td>\n",
       "      <td>0.801375</td>\n",
       "      <td>0.800251</td>\n",
       "      <td>0.800251</td>\n",
       "      <td>0.798004</td>\n",
       "      <td>0.798010</td>\n",
       "      <td>0.796887</td>\n",
       "      <td>0.799127</td>\n",
       "      <td>0.800251</td>\n",
       "      <td>0.800251</td>\n",
       "      <td>0.800251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9 rows Ã— 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   hidden neurons  learning parameter         0         1         2         3  \\\n",
       "0              16                 1.2  0.630707  0.658785  0.691325  0.726106   \n",
       "1              16                 1.4  0.495129  0.620721  0.651026  0.685795   \n",
       "2              16                 1.6  0.566669  0.623928  0.649802  0.704758   \n",
       "3              18                 1.2  0.451315  0.628579  0.696918  0.701406   \n",
       "4              18                 1.4  0.491777  0.610502  0.615096  0.673398   \n",
       "5              18                 1.6  0.408618  0.667742  0.682336  0.711550   \n",
       "6              20                 1.2  0.501494  0.543149  0.607194  0.677905   \n",
       "7              20                 1.4  0.558778  0.613828  0.669996  0.710426   \n",
       "8              20                 1.6  0.459174  0.603754  0.624079  0.679047   \n",
       "\n",
       "          4         5         6         7    ...           41        42  \\\n",
       "0  0.731762  0.737393  0.749733  0.760950    ...     0.800232  0.800232   \n",
       "1  0.716088  0.728416  0.752018  0.753135    ...     0.791250  0.791250   \n",
       "2  0.713778  0.744071  0.754190  0.754177    ...     0.788996  0.790120   \n",
       "3  0.728360  0.744065  0.751924  0.765407    ...     0.791250  0.792373   \n",
       "4  0.709334  0.730657  0.746369  0.764327    ...     0.796855  0.797979   \n",
       "5  0.713797  0.723903  0.727274  0.747505    ...     0.785632  0.784508   \n",
       "6  0.704890  0.718348  0.728441  0.736269    ...     0.786768  0.787892   \n",
       "7  0.733984  0.755339  0.758684  0.771038    ...     0.804720  0.803597   \n",
       "8  0.704921  0.731800  0.727368  0.753135    ...     0.801375  0.800251   \n",
       "\n",
       "         43        44        45        46        47        48        49  \\\n",
       "0  0.800232  0.800232  0.800232  0.800232  0.800232  0.799109  0.799109   \n",
       "1  0.790126  0.791243  0.790126  0.790126  0.790126  0.790126  0.789003   \n",
       "2  0.788996  0.788996  0.788996  0.788996  0.788996  0.791243  0.793491   \n",
       "3  0.791250  0.790126  0.790126  0.790126  0.790126  0.791250  0.790132   \n",
       "4  0.797979  0.797979  0.797979  0.796855  0.796855  0.795732  0.795732   \n",
       "5  0.785626  0.784502  0.785626  0.786749  0.786749  0.786749  0.787873   \n",
       "6  0.787892  0.789015  0.789015  0.789015  0.790139  0.790139  0.790139   \n",
       "7  0.803597  0.804720  0.804720  0.804720  0.804720  0.804720  0.804720   \n",
       "8  0.800251  0.798004  0.798010  0.796887  0.799127  0.800251  0.800251   \n",
       "\n",
       "         50  \n",
       "0  0.799109  \n",
       "1  0.787879  \n",
       "2  0.794614  \n",
       "3  0.790132  \n",
       "4  0.795732  \n",
       "5  0.787873  \n",
       "6  0.790139  \n",
       "7  0.804720  \n",
       "8  0.800251  \n",
       "\n",
       "[9 rows x 53 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "averages=performance_data.groupby(['hidden neurons','learning parameter']).mean().reset_index()\n",
    "averages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum value in the table of averages is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8080848659844001"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "averages.loc[:,list(range(51))].max().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to figure out in which column the maximum is located in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "averages.loc[:,list(range(51))].max().idxmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the row it's in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "averages.loc[:,29].idxmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the maximum score was obtained in row 0 at column 29.  Double checking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8080848659844001"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "averages.loc[0,29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hidden neurons        16.0\n",
       "learning parameter     1.2\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "averages.loc[0,['hidden neurons','learning parameter']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, this test would seem to suggest that our network behaved the best on the data when there were 16 hidden neurons and a learning rate of 1.2, at epoch 29.  We could then train a network with this configuration on the entire titanic training data set, then compute the predictions of the test data and submit to Kaggle to see what we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic_net=neural_net(9,16,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic_net.fit(X,T,1.2,29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Emb1</th>\n",
       "      <th>Emb2</th>\n",
       "      <th>Emb3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.872436</td>\n",
       "      <td>-0.755024</td>\n",
       "      <td>0.385769</td>\n",
       "      <td>-0.498872</td>\n",
       "      <td>-0.399769</td>\n",
       "      <td>-0.496818</td>\n",
       "      <td>-0.567462</td>\n",
       "      <td>-1.349059</td>\n",
       "      <td>2.840354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.872436</td>\n",
       "      <td>1.321292</td>\n",
       "      <td>1.369729</td>\n",
       "      <td>0.616254</td>\n",
       "      <td>-0.399769</td>\n",
       "      <td>-0.511665</td>\n",
       "      <td>-0.567462</td>\n",
       "      <td>0.739484</td>\n",
       "      <td>-0.351227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.315441</td>\n",
       "      <td>-0.755024</td>\n",
       "      <td>2.550481</td>\n",
       "      <td>-0.498872</td>\n",
       "      <td>-0.399769</td>\n",
       "      <td>-0.463545</td>\n",
       "      <td>-0.567462</td>\n",
       "      <td>-1.349059</td>\n",
       "      <td>2.840354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.872436</td>\n",
       "      <td>-0.755024</td>\n",
       "      <td>-0.204607</td>\n",
       "      <td>-0.498872</td>\n",
       "      <td>-0.399769</td>\n",
       "      <td>-0.481898</td>\n",
       "      <td>-0.567462</td>\n",
       "      <td>0.739484</td>\n",
       "      <td>-0.351227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.872436</td>\n",
       "      <td>1.321292</td>\n",
       "      <td>-0.598191</td>\n",
       "      <td>0.616254</td>\n",
       "      <td>0.619154</td>\n",
       "      <td>-0.416992</td>\n",
       "      <td>-0.567462</td>\n",
       "      <td>0.739484</td>\n",
       "      <td>-0.351227</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pclass       Sex       Age     SibSp     Parch      Fare      Emb1  \\\n",
       "0  0.872436 -0.755024  0.385769 -0.498872 -0.399769 -0.496818 -0.567462   \n",
       "1  0.872436  1.321292  1.369729  0.616254 -0.399769 -0.511665 -0.567462   \n",
       "2 -0.315441 -0.755024  2.550481 -0.498872 -0.399769 -0.463545 -0.567462   \n",
       "3  0.872436 -0.755024 -0.204607 -0.498872 -0.399769 -0.481898 -0.567462   \n",
       "4  0.872436  1.321292 -0.598191  0.616254  0.619154 -0.416992 -0.567462   \n",
       "\n",
       "       Emb2      Emb3  \n",
       "0 -1.349059  2.840354  \n",
       "1  0.739484 -0.351227  \n",
       "2 -1.349059  2.840354  \n",
       "3  0.739484 -0.351227  \n",
       "4  0.739484 -0.351227  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df=format_data('titanic_test.csv',predictors)\n",
    "test_full_data=pd.read_csv('titanic_test.csv')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test=test_df[predictors].values.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions=np.round_(titanic_net.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  1.,\n",
       "         0.,  1.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,  1.,  1.,  0.,\n",
       "         1.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,  1.,  1.,  0.,\n",
       "         0.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,\n",
       "         1.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  1.,\n",
       "         1.,  1.,  0.,  0.,  1.,  1.,  0.,  1.,  0.,  1.,  1.,  0.,  1.,\n",
       "         0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  0.,  1.,\n",
       "         0.,  1.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,\n",
       "         1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,  0.,  0.,\n",
       "         1.,  1.,  1.,  1.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,  1.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,\n",
       "         1.,  1.,  0.,  1.,  1.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,  1.,\n",
       "         1.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  1.,  1.,  0.,  0.,\n",
       "         1.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  1.,  1.,  0.,  1.,  1.,  0.,  1.,  1.,  0.,  0.,  1.,  0.,\n",
       "         1.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  1.,  0.,  1.,\n",
       "         0.,  1.,  0.,  1.,  1.,  0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  1.,\n",
       "         0.,  1.,  1.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,\n",
       "         0.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,\n",
       "         1.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.,  1.,  1.,  1.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "         0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  1.,  1.,\n",
       "         0.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,\n",
       "         1.,  1.,  0.,  1.,  1.,  0.,  0.,  1.,  0.,  0.,  1.,  1.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,\n",
       "         0.,  1.,  0.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  1.,  0.,  1.,\n",
       "         0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  0.,  1.,  0.,\n",
       "         0.,  0.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction_data=pd.DataFrame({'PassengerId':test_full_data['PassengerId'],'Survived':predictions[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction_data.Survived=prediction_data.Survived.astype(int)  # Important: the predictions we made above are floats,\n",
    "                                                               # while Kaggle wants them as integers, so we must convert\n",
    "                                                               # them to integers unless we want to get a score of 0.00."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived\n",
       "0          892         0\n",
       "1          893         0\n",
       "2          894         0\n",
       "3          895         0\n",
       "4          896         1"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction_data.to_csv('titanic_predictions_practice.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The above predictions only scored 0.75598 on the Kaggle data, so definitely not as high as the cross validation data we collected above might have indicated.  We could repeat the proceedure above with a wider range of hidden neurons, learning rates, and epochs tested, or we could repeat it with the ```perf_test2``` function to test out networks with 2 hidden layers.  The best I've scored using these methods is 0.78469."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
